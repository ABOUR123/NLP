{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe44a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge jupyterlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b0f589",
   "metadata": {},
   "outputs": [],
   "source": [
    "conda update -n base -c defaults conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01663ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "result = nlp(\"I hate you\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n",
    "\n",
    "result = nlp(\"I love you\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b498a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4bb9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "result = nlp(\"I hate you\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")\n",
    "\n",
    "result = nlp(\"I love you\")[0]\n",
    "print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1243563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f56f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "nlp = pipeline(\"sentiment-analysis\",model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "result = nlp(\"I hate you\")\n",
    "print(result)\n",
    "\n",
    "result = nlp(\"I love you\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c92e5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "nlp = pipeline(\"sentiment-analysis\",model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "result = nlp(\"I hate you\")\n",
    "print(result)\n",
    "\n",
    "result = nlp(\"I love you\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09f9d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "nlp = pipeline(\"sentiment-analysis\",model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "result = nlp(\"I hate you\")\n",
    "print(result)\n",
    "\n",
    "result = nlp(\"I love you\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a5a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6426c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78280f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "nlp = pipeline(\"sentiment-analysis\",model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "result = nlp(\"I hate you\")\n",
    "print(result)\n",
    "\n",
    "result = nlp(\"I love you\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d44fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio===0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a95b63b",
   "metadata": {},
   "source": [
    "# Sentiment analysis :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "977660da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': '1 star', 'score': 0.634607195854187}]\n",
      "[{'label': '5 stars', 'score': 0.8546806573867798}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline(\"sentiment-analysis\",model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "result = nlp(\"I hate you\")\n",
    "print(result)\n",
    "\n",
    "result = nlp(\"I love you\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fdee92",
   "metadata": {},
   "source": [
    "# text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7e2aafa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'Hello how are  you? Hello, I am a \"new\" blogger. I am a \"new\" blogger. I am a \"new\" blogger. I am a \"new\" blogger. I am a \"'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'مرحبا كيف حالك؟ كيف يمكن ان يكون هناك بعض من الممكن ان تكون هناك بعض من الممكن ان تكون هذه الرواية من الممكن ان تكون افضل من ذلك\"\\n\"لم تعجبني كثيراً و لكن لم تعجبني بعض القصص التي أعجبتني و لكن لم تعجبني بعض القصص'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_generator_ENG = pipeline(\"text-generation\",model=\"xlnet-base-cased\")\n",
    "print(text_generator_ENG(\"Hello how are \", max_length=50, do_sample=False))\n",
    "\n",
    "text_generator_AR = pipeline(\"text-generation\",model=\"mofawzy/gpt2-arabic-sentence-generator\")\n",
    "print(text_generator_AR(\"مرحبا كيف حالك\", max_length=50, do_sample=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d7dfed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552650ac423b45cbbb0c10a0fdc39c07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.31k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54a18eff4b2423da669d1d327585632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f0242d5f56c49e9b945c554bb644228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b312e25692f641e997028b74ff31671f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/974k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda026a391064b23a65f9e73f1aa7134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/532k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327bab6ef6214664a62538d449c3da5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'bonsoir tous le monde ous avez des idées ? Il est donc important de bien choisir son matériel . Il est donc important de bien choisir son matériel . Il est donc important de bien choisir son matériel . Il est donc important de bien choisir son matériel'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_generator_FR = pipeline(\"text-generation\",model=\"antoiloui/belgpt2\")\n",
    "print(text_generator_FR(\"bonsoir tous le monde \", max_length=50, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec6c0a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'je vais partir en vacances , je vais me faire un petit plaisir , je vais faire un petit tour sur le blog de ma copine , je vais faire un petit tour sur le tien , je vais faire un petit tour sur le tien , je vais faire'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_generator_FR = pipeline(\"text-generation\",model=\"antoiloui/belgpt2\")\n",
    "print(text_generator_FR(\"je vais partir\", max_length=50, do_sample=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1baa6b8",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "056b1e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d84c19cf484f4b8076d97174935e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/998 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d823bd07cf84865846d44d09be287a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2764e237680d41a6bb1e8c27fa5c65b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/60.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464c504942f54d2cb35ca83d6ed50919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    ">>> from transformers import pipeline\n",
    "\n",
    ">>> nlp = pipeline(\"ner\")\n",
    "\n",
    ">>> sequence = \"Une plage est une berge en pente douce ou très douce1, ainsi que, dans une acception plus étendue qui tend de plus en plus à être d'ordre maritime depuis le xixe siècle, un rivage oblique, assez peu prononcé par rapport à l'horizontale, qui se poursuit longuement sous le niveau de l'eau2. Cette morphologie de la berge ou du rivage par rapport au plan d'eau, à la rivière ou à la mer favorisent l'échouage des embarcations ou des navires, comme l'atterrissement des corps et matériaux transportés par les flots ou poussés par les courants\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b36ccf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print (nlp(sequence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae89ddbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'I-PER', 'score': 0.9992354, 'index': 1, 'word': 'Sa', 'start': 0, 'end': 2}, {'entity': 'I-PER', 'score': 0.998697, 'index': 2, 'word': '##ad', 'start': 2, 'end': 4}, {'entity': 'I-PER', 'score': 0.99874896, 'index': 3, 'word': 'Din', 'start': 5, 'end': 8}, {'entity': 'I-PER', 'score': 0.9979856, 'index': 4, 'word': '##e', 'start': 8, 'end': 9}, {'entity': 'I-PER', 'score': 0.99563384, 'index': 5, 'word': 'El', 'start': 10, 'end': 12}, {'entity': 'I-PER', 'score': 0.9975122, 'index': 6, 'word': 'O', 'start': 13, 'end': 14}, {'entity': 'I-PER', 'score': 0.9762487, 'index': 7, 'word': '##tman', 'start': 14, 'end': 18}, {'entity': 'I-PER', 'score': 0.95954144, 'index': 8, 'word': '##i', 'start': 18, 'end': 19}, {'entity': 'I-LOC', 'score': 0.9889963, 'index': 19, 'word': 'In', 'start': 45, 'end': 47}, {'entity': 'I-LOC', 'score': 0.94216764, 'index': 20, 'word': '##ez', 'start': 47, 'end': 49}, {'entity': 'I-LOC', 'score': 0.97026837, 'index': 21, 'word': '##gan', 'start': 49, 'end': 52}, {'entity': 'I-LOC', 'score': 0.97270185, 'index': 22, 'word': '##e', 'start': 52, 'end': 53}]\n"
     ]
    }
   ],
   "source": [
    "nlp = pipeline(\"ner\")\n",
    "\n",
    "sequence = \"Saad Dine El Otmani, né le 16 janvier 1956 à Inezgane\"\n",
    "print (nlp(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c88cd86",
   "metadata": {},
   "source": [
    "# Question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "238b59d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc53c18d6f71456596053c21b9c02266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4b71e5ae38498a9dedc03f043a10f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3216bc24b989457bb9cf8ffc44173c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d60db5287b854f69a8dfdfd871461734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62473f91020d48d49a04e9fa7e889fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question_answerer = pipeline('question-answering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c30d2a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.05075601115822792,\n",
       " 'start': 53,\n",
       " 'end': 99,\n",
       " 'answer': 'a group that is usually longer than a sentence'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_en = \"A paragraph is a group of words put together to form a group that is usually longer than a sentence. Paragraphs are often made up of several sentences. There are usually between three and eight sentences. Paragraphs can begin with an indentation (about five spaces), or by missing a line out, and then starting again. This makes it easier to see when one paragraph ends and another begins.\"\n",
    "\n",
    "answer_question_en = question_answerer(question = \"What is paragraph\", context = context_en)\n",
    "answer_question_en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cab046",
   "metadata": {},
   "source": [
    "# Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "735410b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a2680ed5c244e349f8e4654a0cef9a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.80k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ab0250e97b450a9e34886ae6091db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a19c9e09131d4d458e487ba634d888dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a9825a2b454c1db4290b12ced6c5d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "437694f1ff7c4af8b1cb9c0c0fb50f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADIL\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': \" Une plage is a berge en pente douce ou très douce, ainsi that tend de plus en plus à être d'ordre maritime depuis le xixe siècle . A rivage oblique, assez peu prononcé par rapport à l'horizontale, se poursuit long sous le niveau de l'eau .\"}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "ARTICLE = \"Une plage est une berge en pente douce ou très douce1, ainsi que, dans une acception plus étendue qui tend de plus en plus à être d'ordre maritime depuis le xixe siècle, un rivage oblique, assez peu prononcé par rapport à l'horizontale, qui se poursuit longuement sous le niveau de l'eau2. Cette morphologie de la berge ou du rivage par rapport au plan d'eau, à la rivière ou à la mer favorisent l'échouage des embarcations ou des navires, comme l'atterrissement des corps et matériaux transportés par les flots ou poussés par les courants.\"\n",
    "print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afc1a11",
   "metadata": {},
   "source": [
    "# Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8727fc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Un artisanat ou un métier est un passe-temps ou une profession qui exige des compétences et des connaissances particulières en matière de travail qualifié.'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_en_to_fr\")\n",
    "print(translator(\"A craft or trade is a pastime or an occupation that requires particular skills and knowledge of skilled work. In a historical sense\", max_length=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05b43b6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The task does not provide any default models for options ('en', 'ar')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-ac64968b2c72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtranslator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"translation_en_to_ar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"A craft or trade is a pastime or an occupation that requires particular skills and knowledge of skilled work. In a historical sense\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;31m# At that point framework might still be undetermined\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_default_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtargeted_task\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframework\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtask_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[1;31m# Config is the primordial information item.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\base.py\u001b[0m in \u001b[0;36mget_default_model\u001b[1;34m(targeted_task, framework, task_options)\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtask_options\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtask_options\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"The task does not provide any default models for options {task_options}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[0mdefault_models\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtask_options\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[1;34m\"model\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdefaults\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The task does not provide any default models for options ('en', 'ar')"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "translator = pipeline(\"translation_en_to_ar\")\n",
    "print(translator(\"A craft or trade is a pastime or an occupation that requires particular skills and knowledge of skilled work. In a historical sense\", max_length=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2021b132",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-75cea63abf92>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfr_to_ar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"translation_fr_to_ar\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Helsinki-NLP/opus-mt-fr-ar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mresult_ar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfr_to_ar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"comment ça va ?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mresult_ar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[0mtokenizer_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m             tokenizer = AutoTokenizer.from_pretrained(\n\u001b[0m\u001b[0;32m    443\u001b[0m                 \u001b[0mtokenizer_identifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_fast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_from_pipeline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    566\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m                     raise ValueError(\n\u001b[0m\u001b[0;32m    569\u001b[0m                         \u001b[1;34m\"This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m                         \u001b[1;34m\"in order to use this tokenizer.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer."
     ]
    }
   ],
   "source": [
    "fr_to_ar = pipeline(\"translation_fr_to_ar\", model=\"Helsinki-NLP/opus-mt-fr-ar\")\n",
    "result_ar = fr_to_ar(\"comment ça va ?\")\n",
    "result_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6be419f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91823c2b904f43b5ac553183885973e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed8262db3d64a62b05c4bb2dd6146a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/301M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "574f91095eec48fd9670ac74390d280d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-7e6a9229ae37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfr_to_en\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"translation_fr_to_en\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Helsinki-NLP/opus-mt-fr-en\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mresult_en\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfr_to_en\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"comment ça va ?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mresult_en\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\pipelines\\__init__.py\u001b[0m in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    440\u001b[0m                 \u001b[0mtokenizer_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 442\u001b[1;33m             tokenizer = AutoTokenizer.from_pretrained(\n\u001b[0m\u001b[0;32m    443\u001b[0m                 \u001b[0mtokenizer_identifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_fast\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_from_pipeline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtokenizer_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    566\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m                     raise ValueError(\n\u001b[0m\u001b[0;32m    569\u001b[0m                         \u001b[1;34m\"This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m                         \u001b[1;34m\"in order to use this tokenizer.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer."
     ]
    }
   ],
   "source": [
    "fr_to_en = pipeline(\"translation_fr_to_en\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
    "result_en = fr_to_en(\"comment ça va ?\")\n",
    "result_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bf8f19d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADIL\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3277: FutureWarning: `prepare_seq2seq_batch` is deprecated and will be removed in version 5 of 🤗 Transformers. Use the regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` context manager to prepare your targets. See the documentation of your specific tokenizer for more details\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['كيف تكون']\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"marefa-nlp/marefa-mt-en-ar\")\n",
    "model = MarianMTModel.from_pretrained(\"marefa-nlp/marefa-mt-en-ar\")\n",
    "\n",
    "text = \"how are you\"\n",
    "\n",
    "translated_tokens = model.generate(**tokenizer.prepare_seq2seq_batch(text, return_tensors=\"pt\"))\n",
    "Output_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "\n",
    "print(Output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f008c459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "Installing collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.96\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "505ce96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['كيف تكون']\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianTokenizer, MarianMTModel\n",
    "\n",
    "tokenizer = MarianTokenizer.from_pretrained(\"marefa-nlp/marefa-mt-en-ar\")\n",
    "model = MarianMTModel.from_pretrained(\"marefa-nlp/marefa-mt-en-ar\")\n",
    "\n",
    "text = \"how are you\"\n",
    "\n",
    "translated_tokens = model.generate(**tokenizer.prepare_seq2seq_batch(text, return_tensors=\"pt\"))\n",
    "Output_text = [tokenizer.decode(t, skip_special_tokens=True) for t in translated_tokens]\n",
    "\n",
    "print(Output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c4eb3f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41391421b3d847f8a7ebe666d169dd9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8db8baeb04541f6a516b32bc846a23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88fbbf4a0975444a894dc751e8b51e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf7bfd8e30549aeb5ec34289bd14753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/649 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cdcb4f714754d8f9cfbd42eb2bf9932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/529 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'pprint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-7d3cf5591d7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mencoded_ar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_ar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mgenerated_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mencoded_ar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforced_bos_token_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlang_code_to_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"en_XX\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mpprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerated_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pprint' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "text_ar = \"كيف تكون\"\n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer.src_lang = \"ar_AR\"\n",
    "encoded_ar = tokenizer(text_ar, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**encoded_ar, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"])\n",
    "pprint(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aeb0db1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['How would you be?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "text_ar = \"كيف تكون\"\n",
    "\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer.src_lang = \"ar_AR\"\n",
    "encoded_ar = tokenizer(text_ar, return_tensors=\"pt\")\n",
    "generated_tokens = model.generate(**encoded_ar, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"])\n",
    "print(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9569c3d8",
   "metadata": {},
   "source": [
    "# Fill mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73c15c28",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-748ecb26db7d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfill_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"fill-mask\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"fill-mask\")\n",
    "\n",
    "from pprint import pprint\n",
    "print(nlp(f\"Les coronavirus sont des {nlp.tokenizer.mask_token} de la famille des Coronaviridae.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f8a5317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADIL\\anaconda3\\lib\\site-packages\\torchaudio\\backend\\utils.py:67: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7395805a7ffe4ed0a634495dc004d1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/480 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca2fed68e4847209616d78d16e51a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/331M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c6698efdf1458bb1a5651ae00bec5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c92be71b3a246839cacc703c4abf355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79d8f94e80d44d65b6349915a768019a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'sequence': 'Les coronavirus sont des fins de la famille des Coronaviridae.', 'score': 0.07141322642564774, 'token': 40863, 'token_str': ' fins'}, {'sequence': 'Les coronavirus sont des mutations de la famille des Coronaviridae.', 'score': 0.06041447073221207, 'token': 28513, 'token_str': ' mutations'}, {'sequence': 'Les coronavirus sont des clones de la famille des Coronaviridae.', 'score': 0.0468272864818573, 'token': 44001, 'token_str': ' clones'}, {'sequence': 'Les coronavirus sont des parasites de la famille des Coronaviridae.', 'score': 0.042839255183935165, 'token': 37891, 'token_str': ' parasites'}, {'sequence': 'Les coronavirus sont des genes de la famille des Coronaviridae.', 'score': 0.041360143572092056, 'token': 14819, 'token_str': ' genes'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "nlp = pipeline(\"fill-mask\")\n",
    "\n",
    "from pprint import pprint\n",
    "print(nlp(f\"Les coronavirus sont des {nlp.tokenizer.mask_token} de la famille des Coronaviridae.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d485e5",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "734278cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['engineer',\n",
       " 'good',\n",
       " 'interested',\n",
       " 'politics',\n",
       " 'sample',\n",
       " 'sentence',\n",
       " 'software']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"This is a sample sentence\",\n",
    "    \"I am interested in politics\",\n",
    "    \"You are a very good software engineer, engineer.\",]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "vectorizer.fit(sentences)\n",
    "\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3300c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
